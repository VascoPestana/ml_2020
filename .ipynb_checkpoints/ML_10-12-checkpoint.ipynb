{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import ceil\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for better resolution plots\n",
    "%config InlineBackend.figure_format = 'retina' # optionally, you can change 'svg' to 'retina'\n",
    "\n",
    "# Setting seaborn style\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bf0d3607078c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get the dataset and checking its first rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'Data/Train.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m                 )\n\u001b[0;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine)\u001b[0m\n\u001b[0;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\io\\excel\\_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \"\"\"\n\u001b[0;32m     20\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"xlrd\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\compat\\_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, raise_on_missing, on_version)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraise_on_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd."
     ]
    }
   ],
   "source": [
    "# Get the dataset and checking its first rows\n",
    "df_train = pd.read_excel(r'Data/Train.xlsx')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking data types and nulls in the dataset --> see there are no nulls\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Birthday is type object, so we're transforming it to type datetime\n",
    "# For that we must first fix the problem of having February 29 for non-leap years --> turn all February 29 to February 28\n",
    "df_train['Birthday']=df_train['Birthday'].map(lambda x: x.replace(\"February 29\", \"February 28\"))\n",
    "\n",
    "# Check if the replacement worked\n",
    "df_train['Birthday'][df_train['Birthday'].str.contains(\"February 29\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the format the date appears and the data type to datetime\n",
    "df_train['Birthday']=df_train['Birthday'].map(lambda x: datetime.strptime(x, \" %B %d,%Y\").date())\n",
    "df_train['Birthday'] = pd.to_datetime(df_train['Birthday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a descriptive overview of the variables (both numeric and categorical)\n",
    "df_train.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Citizen_ID as the index\n",
    "df_train.set_index(\"CITIZEN_ID\", inplace=True)\n",
    "\n",
    "# Define Income variable as the target and remove it from the dataframe with the independent variables\n",
    "target = df_train['Income']\n",
    "df_train = df_train.drop(['Income'], axis=1)\n",
    "\n",
    "# Define metric and non-metric datasets\n",
    "metric= df_train.loc[:, np.array(df_train.dtypes==\"int64\")]\n",
    "non_metric= df_train.loc[:,np.array(df_train.dtypes==\"object\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the year from Birthday, so as to get a better visualization of the values\n",
    "metric['Birthday']=df_train.Birthday.map(lambda x: x.year)\n",
    "\n",
    "# Remove Name from the list of non_metric variables since that gives us no meaning or valuable information and plotting it would be useless\n",
    "non_metric.drop(columns=\"Name\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking metrics variables' distribution and pairwise relationship\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Setting pairgrid\n",
    "g = sns.PairGrid(metric)\n",
    "\n",
    "# Populating pairgrid\n",
    "mdg = g.map_diag(plt.hist, edgecolor=\"w\", color=\"peru\")\n",
    "mog = g.map_offdiag(plt.scatter, edgecolor=\"w\", color=\"peru\", s=40)\n",
    "    \n",
    "# Layout\n",
    "plt.subplots_adjust(top=0.92)\n",
    "plt.suptitle(\"Pairwise relationship of metric variables\", fontsize=25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciona\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(30,40))\n",
    "\n",
    "ax1=sns.countplot(non_metric[\"Native Continent\"], ax=axes[0,0])\n",
    "ax2=sns.countplot(non_metric[\"Lives with\"], ax=axes[0,1])\n",
    "ax3=sns.countplot(non_metric[\"Marital Status\"], ax=axes[1,0])\n",
    "ax4=sns.countplot(non_metric[\"Base Area\"], ax=axes[1,1])\n",
    "ax5=sns.countplot(non_metric[\"Employment Sector\"], ax=axes[2,0])\n",
    "ax6=sns.countplot(non_metric[\"Education Level\"], ax=axes[2,1])\n",
    "ax7=sns.countplot(non_metric[\"Role\"], ax=axes[3,0])\n",
    "# ax8=sns.countplot(non_metric[\"Birthday\"], ax=axes[3,1])\n",
    "\n",
    "ax1.tick_params(labelsize=17)\n",
    "ax1.set_xlabel(xlabel='Native Continent',fontsize = 19)\n",
    "\n",
    "ax2.tick_params(labelsize=17)\n",
    "ax2.set_xlabel(xlabel='Marital Status',fontsize = 19)\n",
    "\n",
    "ax3.tick_params(labelsize=17)\n",
    "ax3.set_xticklabels(ax3.get_xticklabels(), rotation=90)\n",
    "ax3.set_xlabel(xlabel='Lives With',fontsize = 19)\n",
    "\n",
    "ax4.tick_params(labelsize=17)\n",
    "ax4.set_xticklabels(ax4.get_xticklabels(), rotation=90)\n",
    "ax4.set_xlabel(xlabel='Base Area',fontsize = 19)\n",
    "\n",
    "ax5.tick_params(labelsize=17)\n",
    "ax5.set_xticklabels(ax5.get_xticklabels(), rotation=90)\n",
    "ax5.set_xlabel(xlabel='Employment Sector',fontsize = 19)\n",
    "\n",
    "ax6.tick_params(labelsize=17)\n",
    "ax6.set_xticklabels(ax6.get_xticklabels(), rotation=90)\n",
    "ax6.set_xlabel(xlabel='Education Level',fontsize = 19)\n",
    "\n",
    "ax7.tick_params(labelsize=17)\n",
    "ax7.set_xticklabels(ax7.get_xticklabels(), rotation=90)\n",
    "ax7.set_xlabel(xlabel='Role',fontsize = 19)\n",
    "\n",
    "# ax8.tick_params(labelsize=17)\n",
    "# ax8.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "# ax8.set_xlabel(xlabel='Birthday',fontsize = 19)\n",
    "\n",
    "plt.subplots_adjust(top=0.95,hspace=0.75)\n",
    "plt.suptitle(\"Distribution of non-metric variables\", fontsize=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the plots above, we can see the distribution of the values per category, for each categorical variables.\n",
    "# With this, we see that there are 3 variables containing \"?\" as a value, which we understand are be null values\n",
    "# So, here we are replacing those ? for null values, to analyze them more efficiently\n",
    "df_train=df_train.replace('?', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can already see how many missing values each variable has\n",
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inutil mas func\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = metric.corr() #Getting correlation of numerical variables\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool) #Return an array of zeros (Falses) with the same shape and type as a given array\n",
    "mask[np.triu_indices_from(mask)] = True #The upper-triangle array is now composed by True values\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True) #Make a diverging palette between two HUSL colors. Return a matplotlib colormap object.\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, center=0, square=True, linewidths=.5, ax=ax, annot=True)\n",
    "\n",
    "# Layout\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.suptitle(\"Correlation matrix\", fontsize=20)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=90)\n",
    "# Fixing the bug of partially cut-off bottom and top cells\n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "df_train.groupby([\"Base Area\",\"Role\"])[\"Role\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_train['Base Area'], df_train['Role'],  margins = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_train['Marital Status'], df_train['Lives with'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MUDAR\n",
    "#Check if there is someone with more than 70 years that has Employment Sector not null\n",
    "df_train[(metric.Birthday<1978)]\n",
    "#& (metric[\"Money Received\"]==0)] that did not receive money to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is anyone born after the year of this experiment (2048) \n",
    "len(df_train[(metric.Birthday>2048)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the oldest year of birth and most recent one\n",
    "print(metric.Birthday.min(),metric.Birthday.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are negative amounts of money\n",
    "df_train[(df_train[\"Money Received\"]<0)|(df_train[\"Ticket Price\"]<0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is anyone who paid for the ticket and at the same time received money to join the experiment\n",
    "df_train[(df_train[\"Ticket Price\"]!=0) & (df_train[\"Money Received\"]!=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is anyone with a certain level of education and years of education that don't match at all\n",
    "df_train.groupby([\"Years of Education\",\"Education Level\"])[\"Years of Education\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Education Level']=df_train['Education Level'].replace('Preschool', 'No Relevant Education')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Education Level\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Years of Education']=df_train['Years of Education'].replace(2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Years of Education\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is anyone whose marital status seems incoherent with the person he/she lives with someone\n",
    "df_train.groupby([\"Marital Status\",\"Lives with\"] )[\"Lives with\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AQUIII\n",
    "\n",
    "ola=df_train.copy()\n",
    "ola[\"age\"]=2048-ola.Birthday.map(lambda x: x.year)\n",
    "ola[ola.Birthday.map(lambda x: 2048-x.year)<(ola[\"Years of Education\"]+5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if there is anyone with more years of education than his/her age\n",
    "df_train[df_train.Birthday.map(lambda x: 2048-x.year)<(df_train[\"Years of Education\"]+5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unemployed citizens have Role and Working Hours different than 0\n",
    "df_train[[\"Employment Sector\", \"Role\", \"Working Hours per week\"]][df_train[\"Employment Sector\"]==\"Unemployed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Never worked citizens have Role and Working Hours different than 0\n",
    "df_train[[\"Employment Sector\", \"Role\", \"Working Hours per week\"]][df_train[\"Employment Sector\"]==\"Never Worked\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Working Hours per week\"][(df_train[\"Employment Sector\"]==\"Unemployed\") | (df_train[\"Employment Sector\"]==\"Never Worked\")]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Role\"][(df_train[\"Employment Sector\"]==\"Never Worked\")]=\"No Role\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[[\"Employment Sector\", \"Role\", \"Working Hours per week\"]][(df_train[\"Employment Sector\"]==\"Unemployed\") | (df_train[\"Employment Sector\"]==\"Never Worked\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "data = pd.melt(metric)\n",
    "plot_features = metric.columns\n",
    "\n",
    "#Prepare figure layout\n",
    "fig, axes = plt.subplots(1, len(plot_features), figsize=(15,8), constrained_layout=True)\n",
    "    \n",
    "# Draw the boxplots\n",
    "for i in zip(axes, plot_features):\n",
    "    sns.boxplot(x=\"variable\", y=\"value\", data=data.loc[data[\"variable\"]==i[1]], ax=i[0], color='peru')\n",
    "    i[0].set_xlabel(\"\")\n",
    "    i[0].set_ylabel(\"\")\n",
    "\n",
    "# Finalize the plot\n",
    "plt.suptitle(\"Metric variables' box plots\", fontsize=25)\n",
    "sns.despine(bottom=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "df_train[df_train[\"Money Received\"]==122999] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Working Hours per week\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#RECOMENTAR\n",
    "\n",
    "#using the IQR method to filter outliers: the scatter plot was impossible to analyze with the outliers, and there are some\n",
    "#very extreme values on 'precoContratual' and, especially, on 'prazoExecucao'\n",
    "\n",
    "#calculate the first and third quantiles:\n",
    "q25 = df_train['Working Hours per week'].quantile(.25)\n",
    "q75 = df_train['Working Hours per week'].quantile(.75)\n",
    "#calculate the inter-quartile range:\n",
    "iqr = (q75 - q25)\n",
    "\n",
    "#calculate the lower and upper fence:\n",
    "multiplicator = 6 #5 or 6 looks better; with 1.5 (the \"official\") or 2 you delete too much of the data\n",
    "upper_lim = q75 + multiplicator * iqr\n",
    "lower_lim = q25 - multiplicator * iqr\n",
    "\n",
    "#filtering the outliers based on the IQR limits\n",
    "df_train_new = df_train[(df_train['Working Hours per week'] > lower_lim) & (df_train['Working Hours per week'] < upper_lim)]\n",
    "print('Percentage of data kept after removing outliers:', np.round(df_train_new.shape[0] / df_train.shape[0], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided not to eliminate outliers on \"Money Received\", since when observing the value that was more distant from the rest, we concluded that there were several people with that high value (122999), all having a relatively important role and high hours of work. \n",
    "\n",
    "Also, we do not consider relevant the amount itself of money received or payed, but instead, if the person received or payed to go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.boxplot(x=\"Working Hours per week\", data=df_train_new, color='peru')\n",
    "plt.suptitle(\"Working Hours Without Outliers\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of citizens that have less than 7.5 years of education to conclude if they might be outliers\n",
    "len(df_train[df_train[\"Years of Education\"]<7.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAO USAMOS\n",
    "\n",
    "# normality test\n",
    "stat, p = shapiro(df_train[\"Working Hours per week\"])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Sample looks Gaussian (fail to reject H0)')\n",
    "else:\n",
    "    print('Sample does not look Gaussian (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st method - impute with the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1=df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = non_metric.mode().loc[0]\n",
    "df_train1.fillna(modes, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd method - imupte with the mode of categories in common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand if certain non-numeric variables are dependent or not from the ones with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that performs the Chi2 test for independence, to check for association between each variable with no missing\n",
    "# values and each of the ones with missings\n",
    "\n",
    "def check_association(col1,col2):\n",
    "    # contingency table\n",
    "    tab=pd.crosstab(df_train[col1], df_train[col2], margins=False).values\n",
    "    stat, p, dof, expected = chi2_contingency(tab)\n",
    "\n",
    "    # interpret test-statistic\n",
    "    prob = 0.95\n",
    "    critical = chi2.ppf(prob, dof)\n",
    "    print('probability=%.3f, critical=%.3f, stat=%.3f' % (prob, critical, stat))\n",
    "    \n",
    "    if abs(stat) >= critical:\n",
    "        print(col1,'and',col2,'are dependent (reject H0).')\n",
    "    else:\n",
    "        print(col1,'and',col2,'are independent (fail to reject H0).')\n",
    "        \n",
    "    # interpret p-value\n",
    "    alpha = 1.0 - prob\n",
    "    print('significance=%.3f, p=%.3f' % (alpha, p))\n",
    "    \n",
    "    if p <= alpha:\n",
    "        print(col1,'and',col2,'are dependent (reject H0).\\n')\n",
    "    else:\n",
    "        print(col1,'and',col2,'are independent (fail to reject H0).\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ebdb6f970ea0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# FIRST: Marital Status\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcheck_association\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Marital Status'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Base Area'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mcheck_association\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Marital Status'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Employment Sector'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcheck_association\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Marital Status'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Role'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-a59d5a093033>\u001b[0m in \u001b[0;36mcheck_association\u001b[1;34m(col1, col2)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcheck_association\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# contingency table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmargins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mstat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdof\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchi2_contingency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "# FIRST: Marital Status\n",
    "\n",
    "check_association('Marital Status','Base Area')\n",
    "check_association('Marital Status','Employment Sector')\n",
    "check_association('Marital Status','Role')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND: Education Level\n",
    "\n",
    "check_association('Education Level','Base Area')\n",
    "check_association('Education Level','Employment Sector')\n",
    "check_association('Education Level','Role')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing both of these variables are associated with the three variables with missing values, using the similarities between people\n",
    "# in these categories, might be useful to discover their characteristics in the missing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the dataframe into another to apply the changes there before\n",
    "df_train2=df_train.copy()\n",
    "\n",
    "# Create a function to impute the missing values by the mode of the records belonging to the same classes of Marital Status and Education Level\n",
    "def impute_mode_by_cat(col):\n",
    "    exp=df_train2[df_train2[col].isnull()].reset_index()\n",
    "    gr=df_train2.groupby([\"Education Level\",\"Marital Status\"])[col].agg(pd.Series.mode)\n",
    "    \n",
    "    # if there are null values in the group by, replace them by the overall mode of the orginal variable\n",
    "    for i in range(len(gr)):\n",
    "        if len(gr[i])==0:\n",
    "            gr[i]=df_train2[col].mode()[0]\n",
    "\n",
    "\n",
    "    # define the values of base area in the new auxiliary dataset as the mode of the base are values for the observations with the\n",
    "    # same level of education and marital status, because at least to some extent, they are more similar than the others       \n",
    "    for i in range(len(exp)):\n",
    "        for x in range(len(gr)):\n",
    "            if (exp['Education Level'][i]==gr.index[x][0]) and (exp['Marital Status'][i]==gr.index[x][1]):\n",
    "                exp[col][i]=gr[x]\n",
    "                \n",
    "    # if it's bimodal (has more than one mode, keep only the first one provided)\n",
    "    for i in range(len(exp)):\n",
    "        if type(exp[col][i])!=str:\n",
    "            exp[col][i]=df_train2[col].mode()[0]\n",
    "\n",
    "    # finally, replace the null values in Base Area in the original dataset by the values acquired before\n",
    "    for i in range(len(exp)):\n",
    "        df_train2.loc[exp['CITIZEN_ID'][i],col]=exp[col][i]\n",
    "        \n",
    "    return df_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2[df_train2['Base Area'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2=impute_mode_by_cat('Base Area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train2[df_train2['Employment Sector'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2=impute_mode_by_cat('Employment Sector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2[df_train2['Role'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2=impute_mode_by_cat('Role')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciona\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(25,25))\n",
    "\n",
    "\n",
    "axa=sns.countplot(df_train2[\"Base Area\"], ax=axes[0,1])\n",
    "axb=sns.countplot(df_train2[\"Employment Sector\"], ax=axes[1,1])\n",
    "axc=sns.countplot(df_train2[\"Role\"], ax=axes[2,1])\n",
    "\n",
    "ax4=sns.countplot(non_metric[\"Base Area\"], ax=axes[0,0])\n",
    "ax5=sns.countplot(non_metric[\"Employment Sector\"], ax=axes[1,0])\n",
    "ax7=sns.countplot(non_metric[\"Role\"], ax=axes[2, 0])\n",
    "\n",
    "\n",
    "axa.tick_params(labelsize=15)\n",
    "axa.set_xticklabels(axa.get_xticklabels(), rotation=90)\n",
    "axa.set_xlabel(xlabel='Base Area',fontsize = 17)\n",
    "axa.set_ylabel(ylabel='Nr of observations',fontsize = 17)\n",
    "axa.set(ylim=(0, 21000))\n",
    "\n",
    "ax4.tick_params(labelsize=15)\n",
    "ax4.set_xticklabels(ax4.get_xticklabels(), rotation=90)\n",
    "ax4.set_xlabel(xlabel='Base Area - with missings',fontsize = 17)\n",
    "ax4.set_ylabel(ylabel='Nr of observations',fontsize = 17)\n",
    "ax4.set(ylim=(0, 21000))\n",
    "\n",
    "axb.tick_params(labelsize=15)\n",
    "axb.set_xticklabels(axb.get_xticklabels(), rotation=90)\n",
    "axb.set_xlabel(xlabel='Employment Sector',fontsize = 17)\n",
    "axb.set_ylabel(ylabel='Nr of observations',fontsize = 17)\n",
    "axb.set(ylim=(0, 17000))\n",
    "\n",
    "ax5.tick_params(labelsize=15)\n",
    "ax5.set_xticklabels(ax5.get_xticklabels(), rotation=90)\n",
    "ax5.set_xlabel(xlabel='Employment Sector - with missings',fontsize = 17)\n",
    "ax5.set_ylabel(ylabel='Nr of observations',fontsize = 17)\n",
    "ax5.set(ylim=(0, 17000))\n",
    "\n",
    "axc.tick_params(labelsize=15)\n",
    "axc.set_xticklabels(axc.get_xticklabels(), rotation=90)\n",
    "axc.set_xlabel(xlabel='Role',fontsize = 17)\n",
    "axc.set_ylabel(ylabel='Nr of observations',fontsize = 17)\n",
    "axc.set(ylim=(0, 3200))\n",
    "\n",
    "ax7.tick_params(labelsize=15)\n",
    "ax7.set_xticklabels(ax7.get_xticklabels(), rotation=90)\n",
    "ax7.set_xlabel(xlabel='Role - with missings',fontsize = 17)\n",
    "ax7.set_ylabel(ylabel='Nr of observations',fontsize = 17)\n",
    "ax7.set(ylim=(0, 3200))\n",
    "\n",
    "plt.subplots_adjust(top=0.9,hspace=0.9)\n",
    "plt.suptitle(\"Distribution of variables with and without missing values\", fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLICARRR\n",
    "\n",
    "# New variable for Age\n",
    "df_train2[\"Age\"]=df_train2.Birthday.map(lambda x: datetime.now().year+28-x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2[[\"Birthday\",\"Age\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New variable for Gender - binary if it's a male or not\n",
    "df_train2['Male'] = np.where(df_train2.Name.str.contains('Mrs|Miss'), '0', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values for variable Marital Status\n",
    "df_train2[\"Marital Status\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In column Marital Status, join \"Married\" with 'Married - Spouse Missing' and 'Married - Spouse in the Army' since we do not consider this distintion relevant\n",
    "df_train2[\"Marital Status_new\"]=df_train2[\"Marital Status\"]\n",
    "df_train2[\"Marital Status_new\"][(df_train2[\"Marital Status\"].str.contains(\"Married\")==True) & (df_train2[\"Marital Status\"]!=\"Married\")]=\"Married\"\n",
    "\n",
    "#In column Marital Status, join \"Divorced\" with 'Separated' in \"Divorced or Separated\"\n",
    "df_train2[\"Marital Status_new\"][(df_train2[\"Marital Status\"]==\"Separated\") | (df_train2[\"Marital Status\"]==\"Divorced\")]=\"Divorced or Separated\"\n",
    "\n",
    "df_train2[\"Marital Status_new\"][(df_train2[\"Marital Status\"]==\"Married\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New binary variable to determine whether the person is married\n",
    "df_train2[\"Is Married\"]='0'\n",
    "df_train2[\"Is Married\"][(df_train2[\"Marital Status_new\"]==\"Married\")]='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2[\"Marital Status_new\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2[\"Is Married\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values for variable Education Level\n",
    "df_train2[\"Education Level\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In column Education Level, join all equal periods of education level\n",
    "\n",
    "df_train2[\"Education Level_new\"]=df_train2[\"Education Level\"]\n",
    "\n",
    "df_train2[\"Education Level_new\"][(df_train2[\"Education Level\"]==\"Middle School - 1st Cycle\") |\n",
    "                    (df_train2[\"Education Level\"]==\"Middle School - 2nd Cycle\")|\n",
    "                    (df_train2[\"Education Level\"]==\"Middle School Complete\")]=\"Middle School\"\n",
    "\n",
    "df_train2[\"Education Level_new\"][(df_train2[\"Education Level\"]==\"High School - 1st Cycle\") | \n",
    "                    (df_train2[\"Education Level\"]==\"High School - 2nd Cycle\") | \n",
    "                    (df_train2[\"Education Level\"]==\"High School Complete\")]=\"High School\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train2[\"Education Level_new\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2[\"Years of Education\"][df_train2[\"Education Level_new\"]=='Professional School'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2[\"Years of Education\"][df_train2[\"Education Level_new\"]=='High School']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join all without considering Post Graduation\n",
    "\n",
    "df_train2[\"Education Level_new2\"]=df_train2[\"Education Level_new\"].copy()\n",
    "df_train2[\"Education Level_new2\"][(df_train2[\"Education Level_new2\"]==\"High School + PostGraduation\")]=\"High School\"\n",
    "df_train2[\"Education Level_new2\"][(df_train2[\"Education Level_new2\"]==\"Bachelors + PostGraduation\")]=\"Bachelors\"\n",
    "df_train2[\"Education Level_new2\"][(df_train2[\"Education Level_new2\"]==\"Professional School + PostGraduation\")]=\"Professional School\"\n",
    "df_train2[\"Education Level_new2\"][(df_train2[\"Education Level_new2\"]==\"Masters + PostGraduation\")]=\"Masters\"\n",
    "df_train2[[\"Education Level_new\", \"Education Level_new2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary that says if the citizen has Post Graduation or not\n",
    "\n",
    "df_train2[\"PostGraduation\"]=df_train2[\"Education Level_new\"].map(lambda x: '1' if \"+\" in x else '0')\n",
    "df_train2[\"PostGraduation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New binary variable related to Higher Education\n",
    "# (includes at least one of the following: Post Graduation, Bachelors, Masters, PhD)\n",
    "\n",
    "df_train2['Higher Education']=np.where(df_train2['Years of Education']>12, '1', '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New binary variable that tells us if each person lives in the capital city or not\n",
    "df_train2['Capital']= np.where(df_train2['Base Area']=='Northbury', '1', '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New binary variable to determine whether the person belongs to Group B (people who were payed to participate in the mission)\n",
    "df_train2['Group B']=np.where(df_train2['Money Received']!=0, '1', '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New binary variable to determine whether the person belongs to Group C (people who payed to participate in the mission)\n",
    "df_train2['Group C']=np.where(df_train2['Ticket Price']!=0, '1', '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-334c9aeef6b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m'Unemployed / Never Worked'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdf_train2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Employment Sector (simplified)'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Employment Sector'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train2' is not defined"
     ]
    }
   ],
   "source": [
    "# publico ou privado ou self ou uneployed empl. sector\n",
    "def sectors(a):\n",
    "    if 'Private Sector' in a:\n",
    "        return 'Private Sector'\n",
    "    elif 'Public Sector' in a:\n",
    "        return 'Public Sector'\n",
    "    #we don't join the Self-Employeds bc the 'Company' ones have much more 1s on the target than the 'Individual'\n",
    "    elif 'Self-Employed (Individual)' in a:\n",
    "        return a\n",
    "    elif 'Self-Employed (Company)' in a:\n",
    "        return a\n",
    "    else:\n",
    "        return 'Unemployed / Never Worked'\n",
    "    \n",
    "df_train2['Employment Sector (simplified)'] = df_train2['Employment Sector'].map(sectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binaria governo ou n empl. sector\n",
    "df_train2['Government'] = df_train2['Employment Sector'].map(lambda x: '1' if 'Government' in x else '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_train2[[\"Ticket Price\", \"Money Received\"]].values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df= pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.histplot(df[0][df[0]!=0]) why???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.histplot(df[1][df[1]!=0]) why???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Median_Money_Received=df_train2[\"Money Received\"][df_train2[\"Money Received\"]>0].median()\n",
    "Median_Ticket_Price=df_train2[\"Ticket Price\"][df_train2[\"Ticket Price\"]>0].median()\n",
    "\n",
    "df_train2['Money Relevance']='0'\n",
    "df_train2['Money Relevance'][df_train2[\"Money Received\"]> Median_Money_Received] ='1'\n",
    "df_train2['Money Relevance'][(df_train2[\"Money Received\"]<= Median_Money_Received) & (df_train2[\"Money Received\"]>0)]='2'\n",
    "\n",
    "df_train2['Money Relevance'][df_train2[\"Ticket Price\"]> Median_Ticket_Price] ='5'\n",
    "df_train2['Money Relevance'][(df_train2[\"Ticket Price\"]<= Median_Ticket_Price) & (df_train2[\"Ticket Price\"]>0)] ='4'\n",
    "\n",
    "df_train2['Money Relevance'][(df_train2[\"Ticket Price\"]== 0) & (df_train2[\"Money Received\"]==0)] ='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2[[\"Ticket Price\", \"Money Received\", \"Money Relevance\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train2.drop(columns=[\"Ticket Price\", \"Money Received\"]), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d9eedf5558e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#interaction between these two: working more hours, has more impact on income with more year of education?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_train2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Working hours * Years of Education\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Working Hours per week\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Years of Education\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "#interaction between these two: working more hours, has more impact on income with more year of education?\n",
    "df_train2[\"Working hours * Years of Education\"] = df_train[\"Working Hours per week\"] * df_train[\"Years of Education\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Money received per year of education\n",
    "df_train2['Money / YE']=0\n",
    "df_train2['Money / YE'][df_train2[\"Years of Education\"]!=0]=round(df_train[\"Money Received\"] / df_train[\"Years of Education\"], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "df_train2['Log 10 of Money Received']=df_train2['Money Received'].map(lambda x: math.log10(x) if x!=0 else 0)\n",
    "df_train2['Log 10 of Ticket Price']=df_train2['Ticket Price'].map(lambda x: math.log10(x) if x!=0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric= df_train2.loc[:,(np.array(df_train2.dtypes==\"int64\")) | (np.array(df_train2.dtypes==\"float64\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize\n",
    "x = metric.values #returns a numpy array\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "stand_metric= pd.DataFrame(x_scaled, columns=metric.columns, index=metric.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = stand_metric.corr() #Getting correlation of numerical variables\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool) #Return an array of zeros (Falses) with the same shape and type as a given array\n",
    "mask[np.triu_indices_from(mask)] = True #The upper-triangle array is now composed by True values\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(20, 12))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True) #Make a diverging palette between two HUSL colors. Return a matplotlib colormap object.\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "#show only corr bigger than 0.6 in absolute value\n",
    "sns.heatmap(corr[(corr>=.7) | (corr<=-.7)], mask=mask, cmap=cmap, center=0, square=True, linewidths=.5, ax=ax)\n",
    "\n",
    "# Layout\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.suptitle(\"Correlation matrix\", fontsize=20)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=90)\n",
    "# Fixing the bug of partially cut-off bottom and top cells\n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between Money Received and Log 10 of Money Received\n",
    "round(corr['Money Received']['Log 10 of Money Received'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#no of features\n",
    "nof_list=np.arange(1,len(stand_metric.columns)+1)  \n",
    "high_score=0\n",
    "\n",
    "#Variable to store the optimum features\n",
    "nof=0           \n",
    "score_list =[]\n",
    "for n in range(len(nof_list)):\n",
    "    # we are going to see in the next class this \"train_test_split()\"...\n",
    "    X_train, X_test, y_train, y_test = train_test_split(stand_metric,target, test_size = 0.3, random_state = 0)\n",
    "    \n",
    "    model = LogisticRegression()\n",
    "    rfe = RFE(model,nof_list[n])\n",
    "    X_train_rfe = rfe.fit_transform(X_train,y_train)\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    model.fit(X_train_rfe,y_train)\n",
    "    \n",
    "    score = model.score(X_test_rfe,y_test)\n",
    "    score_list.append(score)\n",
    "    \n",
    "    if(score>high_score):\n",
    "        high_score = score\n",
    "        nof = nof_list[n]\n",
    "print(\"Optimum number of features: %d\" %nof)\n",
    "print(\"Score with %d features: %f\" % (nof, high_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(estimator = model, n_features_to_select = 7)\n",
    "X_rfe = rfe.fit_transform(X = stand_metric, y = target)\n",
    "model = LogisticRegression().fit(X = X_rfe,y = target)\n",
    "selected_features = pd.Series(rfe.support_, index = stand_metric.columns)\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso\n",
    "def plot_importance(coef,name):\n",
    "    imp_coef = coef.sort_values()\n",
    "    plt.figure(figsize=(8,10))\n",
    "    imp_coef.plot(kind = \"barh\")\n",
    "    plt.title(\"Feature importance using \" + name + \" Model\")\n",
    "    plt.show()\n",
    "    \n",
    "reg = LassoCV()\n",
    "\n",
    "reg.fit(X=stand_metric, y=target)\n",
    "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\n",
    "print(\"Best score using built-in LassoCV: %f\" %reg.score(X = stand_metric,y = target))\n",
    "coef = pd.Series(reg.coef_, index = stand_metric.columns)\n",
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "plot_importance(coef,'Lasso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeClassifierCV().fit(X = stand_metric,y = target)\n",
    "coef_ridge = pd.Series(ridge.coef_[0], index = stand_metric.columns)\n",
    "\n",
    "def plot_importance(coef,name):\n",
    "    imp_coef = coef.sort_values()\n",
    "    plt.figure(figsize=(8,10))\n",
    "    imp_coef.plot(kind = \"barh\")\n",
    "    plt.title(\"Feature importance using \" + name + \" Model\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_importance(coef_ridge,'RidgeClassifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward, backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import tree\n",
    "#from sklearn.model_selection import KFold\n",
    "#import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward = SFS(model, k_features=9, forward=True, scoring=\"accuracy\", cv = None) #floating=False\n",
    "\n",
    "forward.fit(stand_metric, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "forward_table = pd.DataFrame.from_dict(forward.get_metric_dict()).T.drop(columns=['cv_scores', 'ci_bound', 'std_dev', 'std_err'])\n",
    "forward_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_table_max = forward_table['avg_score'].max()\n",
    "forward_table_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forward_table[forward_table['avg_score']==forward_table_max]['feature_names'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward = SFS(model, k_features=1, forward=False, scoring=\"accuracy\", cv = None) #floating=False\n",
    "\n",
    "backward.fit(stand_metric, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "backward_table = pd.DataFrame.from_dict(backward.get_metric_dict()).T.drop(columns=['cv_scores', 'ci_bound', 'std_dev', 'std_err'])\n",
    "backward_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'backward_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-00d3682df88c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbackward_table_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackward_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'avg_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbackward_table_max\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'backward_table' is not defined"
     ]
    }
   ],
   "source": [
    "backward_table_max = backward_table['avg_score'].max()\n",
    "backward_table_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "backward_table[backward_table['avg_score']==backward_table_max]['feature_names'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not useful vars\n",
    "initial_categorical_vars = df_train2.loc[:, np.array(df_train2.dtypes==\"object\")]\n",
    "initial_categorical_vars.drop(columns='Name', inplace=True)\n",
    "initial_categorical_vars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_charts_categorical(df, feature, dep_var):\n",
    "    cont_tab = pd.crosstab(df[feature], dep_var, margins = True)\n",
    "    categories = cont_tab.index[:-1]\n",
    "        \n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    p1 = plt.bar(categories, cont_tab.iloc[:-1, 0].values, 0.55, color=\"gray\")\n",
    "    p2 = plt.bar(categories, cont_tab.iloc[:-1, 1].values, 0.55, bottom=cont_tab.iloc[:-1, 0], color=\"yellowgreen\")\n",
    "    plt.legend((p2[0], p1[0]), ('$y_i=1$', '$y_i=0$'))\n",
    "    plt.title(\"Frequency bar chart\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"$Frequency$\")\n",
    "    #mantemos assim com os 90 graus? ou doutra forma?\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # auxiliary data for 122\n",
    "    obs_pct = np.array([np.divide(cont_tab.iloc[:-1, 0].values, cont_tab.iloc[:-1, 2].values), \n",
    "                        np.divide(cont_tab.iloc[:-1, 1].values, cont_tab.iloc[:-1, 2].values)])\n",
    "      \n",
    "    plt.subplot(122)\n",
    "    p1 = plt.bar(categories, obs_pct[0], 0.55, color=\"gray\")\n",
    "    p2 = plt.bar(categories, obs_pct[1], 0.55, bottom=obs_pct[0], color=\"yellowgreen\")\n",
    "    plt.legend((p2[0], p1[0]), ('$y_i=1$', '$y_i=0$'))\n",
    "    plt.title(\"Proportion bar chart\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"$p$\")\n",
    "    #de novo, mantemos assim com os 90 graus? ou doutra forma?\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial_categorical_features = initial_categorical_vars.columns\n",
    "\n",
    "for i in initial_categorical_features:\n",
    "    bar_charts_categorical(initial_categorical_vars, i, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#por exemplo nas variveis da Education Level, do Marital Status e assim, h variveis que so demasiado redundantes entre\n",
    "#elas, por isso temos de escolher s uma (aquela que distingue melhor os 0s e 1s?)\n",
    "\n",
    "#as melhores variveis categ. parecem ser:\n",
    "#uma das que so do Marital Status\n",
    "#Lives With  boa, mas parece redudante com as de Marital Status (pq as pessoas que moram com o marido ou a mulher  q to com 1s) -> mantemos o marital status\n",
    "#Base Area? fica? por agora no...\n",
    "#uma das que so do Education Level (a ltima?)\n",
    "#uma das que so do Employment Sector?\n",
    "#Male\n",
    "#Group B e C!\n",
    "#Money Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary features that will be droped:\n",
    "# Base Area\n",
    "# Lives with\n",
    "\n",
    "#All these vars will be dropped bc they are too redundant with others:\n",
    "# Marital Status\n",
    "# Is Married\n",
    "# Education Level\n",
    "# PostGraduation\n",
    "# Education Level_new\n",
    "# Employment Sector\n",
    "\n",
    "df_features = initial_categorical_vars.drop(columns=['Marital Status', 'Is Married', 'Lives with', 'Base Area', 'Education Level', 'Education Level_new', 'Employment Sector'])\n",
    "df_features.head()\n",
    "#now, the one hot encoding ->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df_ohc = df_features.drop(columns=['Male', 'Higher Education', 'Capital', 'Group B', 'Group C', 'PostGraduation','Government']).copy()\n",
    "\n",
    "# Use OneHotEncoder to encode the categorical features. Get feature names and create a DataFrame\n",
    "# with the one-hot encoded categorical features (pass feature names)\n",
    "ohc = OneHotEncoder(sparse=False, dtype=int) #MUDAR os x1_ pelo nome da varivel???\n",
    "ohc_feat = ohc.fit_transform(df_ohc)\n",
    "ohc_feat_names = ohc.get_feature_names()\n",
    "ohc_df = pd.DataFrame(ohc_feat, index=df_ohc.index, columns=ohc_feat_names)\n",
    "ohc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importance with all binaries to check the classes with the lowest feature importance on each categorical\n",
    "\n",
    "#feature importance using the split criteria 'Gini'\n",
    "gini_importance = DecisionTreeClassifier().fit(ohc_df, target).feature_importances_\n",
    "\n",
    "#feature importance using the split criteria 'Entropy'\n",
    "entropy_importance = DecisionTreeClassifier(criterion='entropy').fit(ohc_df, target).feature_importances_\n",
    "\n",
    "#plotting the feature importances for both criteria\n",
    "zippy = pd.DataFrame(zip(gini_importance, entropy_importance), columns = ['gini','entropy'])\n",
    "zippy['col'] = ohc_df.columns\n",
    "tidy = zippy.melt(id_vars='col').rename(columns=str.title)\n",
    "tidy.sort_values(['Value'], ascending = False, inplace = True)\n",
    "\n",
    "plt.figure(figsize=(15,20))\n",
    "sns.barplot(y='Col', x='Value', hue='Variable', data=tidy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead of dropping the first, we drop the less \"important\" according to frquency and proportion charts,\n",
    "#and we also checked which class had less feature importance on each categorical feature\n",
    "ohc_df.drop(columns=['x0_Oceania', 'x1_No Role', 'x2_Widow', 'x3_No Relevant Education', 'x4_Unemployed / Never Worked', 'x5_4'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reassigning df to contain ohc variables\n",
    "non_metric_binary = pd.concat([df_features.drop(columns=df_ohc.columns), ohc_df], axis=1)\n",
    "non_metric_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_metric_binary.loc[:, np.array(non_metric_binary.dtypes==\"object\")] = non_metric_binary.loc[:, np.array(non_metric_binary.dtypes==\"object\")].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variables = pd.concat([non_metric_binary, stand_metric], axis=1)\n",
    "all_variables.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'non_metric_binary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c90cf670db8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#feature importance using the split criteria 'Gini'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgini_importance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnon_metric_binary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#feature importance using the split criteria 'Entropy'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mentropy_importance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'entropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnon_metric_binary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'non_metric_binary' is not defined"
     ]
    }
   ],
   "source": [
    "#feature importance using the split criteria 'Gini'\n",
    "gini_importance = DecisionTreeClassifier().fit(non_metric_binary, target).feature_importances_\n",
    "\n",
    "#feature importance using the split criteria 'Entropy'\n",
    "entropy_importance = DecisionTreeClassifier(criterion='entropy').fit(non_metric_binary, target).feature_importances_\n",
    "\n",
    "#plotting the feature importances for both criteria\n",
    "zippy = pd.DataFrame(zip(gini_importance, entropy_importance), columns = ['gini','entropy'])\n",
    "zippy['col'] = non_metric_binary.columns\n",
    "tidy = zippy.melt(id_vars='col').rename(columns=str.title)\n",
    "tidy.sort_values(['Value'], ascending = False, inplace = True)\n",
    "\n",
    "plt.figure(figsize=(15,20))\n",
    "sns.barplot(y='Col', x='Value', hue='Variable', data=tidy)\n",
    "\n",
    "#1. x2_Married\n",
    "#2. x5_1\n",
    "#3. Higher Education\n",
    "#4. x5_3\n",
    "#5. x3_High School\n",
    "#6. Male\n",
    "#7. x1_Management\n",
    "#8. x3_Professional School\n",
    "#9. Capital\n",
    "#10. x5_5\n",
    "#11. x4_Private Sector\n",
    "#12. PostGraduation\n",
    "#13. x0_Europe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest instance, indicating the number of trees\n",
    "rf = RandomForestClassifier(n_estimators = 100, random_state=0, n_jobs=-1)\n",
    "\n",
    "sel = SelectFromModel(rf)\n",
    "\n",
    "#selectFromModel object from sklearn to automatically select the features.\n",
    "sel.fit(non_metric_binary, target)\n",
    "\n",
    "#features with an importance greater than the mean importance of all the features\n",
    "sel.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(non_metric_binary, target)\n",
    "\n",
    "df_imp = pd.DataFrame(rf.feature_importances_, non_metric_binary.columns).reset_index().rename(columns={'index':'binary_variables', 0:'feature_importance'})\n",
    "df_imp.sort_values('feature_importance', ascending=False)\n",
    "\n",
    "#1. x2_Married\n",
    "#2. x5_1\n",
    "#3. x2_Single\n",
    "#4. x5_3\n",
    "#5. Male\n",
    "#6. Higher Education\n",
    "#7. x3_Masters\n",
    "#8. x1_Professor\n",
    "#9. Group B\n",
    "#10. PostGraduation\n",
    "#11. x2_Divorced or Separated\n",
    "#12. x3_Bachelors\n",
    "#13. Capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the selected features on a list and count them\n",
    "selected_feat = non_metric_binary.columns[(sel.get_support())]\n",
    "len(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#names\n",
    "print(selected_feat)\n",
    "\n",
    "#estes esto por ordem de importncia em cima!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazer teste de chi quadrado para a independncia entre as categricas e a target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_target=df_features.copy()\n",
    "df_features_target[\"Target\"]=target\n",
    "df_features_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2 # for chi-squared feature selection\n",
    "\n",
    "X=non_metric_binary\n",
    "y=target\n",
    "\n",
    "sf = SelectKBest(chi2, k='all')\n",
    "sf_fit = sf.fit(X, y)\n",
    "\n",
    "#for i in range(len(sf_fit.scores_)):\n",
    "#    print(' %s: %f' % (X.columns[i], sf_fit.scores_[i]))\n",
    "    \n",
    "# plot the scores\n",
    "datset = pd.DataFrame()\n",
    "datset['feature'] = X.columns[ range(len(sf_fit.scores_))]\n",
    "datset['scores'] = sf_fit.scores_\n",
    "datset = datset.sort_values(by='scores', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,15))\n",
    "sns.barplot(datset['scores'], datset['feature'], color='peru')\n",
    "sns.set_style('whitegrid')\n",
    "plt.ylabel('Categorical Feature', fontsize=18)\n",
    "plt.xlabel('Score', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "#valores mais altos= maior importancia/relevancia p a variavel dependente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MI\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "feat = []\n",
    "mi = []\n",
    "\n",
    "for i in non_metric_binary.columns:\n",
    "    feat.append(i)\n",
    "    \n",
    "    a = np.array(non_metric_binary[i])\n",
    "    b = np.array(target)\n",
    "\n",
    "    #mutual information of 0.69, expressed in nats\n",
    "    mi.append(mutual_info_classif(a.reshape(-1,1), b, discrete_features = True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the MI\n",
    "feat_mi=pd.DataFrame([feat, mi]).T.sort_values(by=1, ascending=False).reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10,15))\n",
    "sns.barplot(x=1, y=0, data=feat_mi, color='peru')\n",
    "sns.set_style('whitegrid')\n",
    "plt.ylabel('Categorical Feature', fontsize=18)\n",
    "plt.xlabel('Mutual Information', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Info among binary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_metric_selected = non_metric_binary[['Male', 'Higher Education', 'Group B', 'x1_Management', 'x1_Professor', 'x2_Married', 'x2_Single', 'x3_Bachelors', 'x3_Masters', 'x5_1', 'x5_3', 'x5_5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "\n",
    "print('Normalized mutual information between binary variables (0-1):\\n')\n",
    "for i in non_metric_selected.columns:\n",
    "    for j in non_metric_selected.columns:\n",
    "        normal_mi = round(normalized_mutual_info_score(non_metric_selected[i], non_metric_selected[j]), 3)\n",
    "        \n",
    "        if i == j: #equals to 1\n",
    "            pass\n",
    "        \n",
    "        elif normal_mi > 0.5:\n",
    "            print(i, 'and', j, ':', normal_mi)\n",
    "            \n",
    "#x3_3 was selected more times than Group B, so we will keep Group x3_3:\n",
    "non_metric_selected.drop(columns='Group B', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixing numerical and categorical variables on a Forward and a Backward Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stand_metric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-a9c2600d77f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstand_metric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Working Hours per week'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Money / YE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Log 10 of Money Received'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Log 10 of Ticket Price'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'stand_metric' is not defined"
     ]
    }
   ],
   "source": [
    "stand_metric.drop(columns=['Working Hours per week', 'Money / YE', 'Log 10 of Money Received', 'Log 10 of Ticket Price'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_selected_variables = pd.concat([non_metric_selected, stand_metric], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_selected_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward = SFS(model, k_features=16, forward=True, scoring=\"accuracy\", cv = None) #floating=False\n",
    "\n",
    "forward.fit(all_selected_variables, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'forward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-aaf46b00d1c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mforward_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metric_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cv_scores'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ci_bound'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'std_dev'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'std_err'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mforward_table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'forward' is not defined"
     ]
    }
   ],
   "source": [
    "forward_table = pd.DataFrame.from_dict(forward.get_metric_dict()).T.drop(columns=['cv_scores', 'ci_bound', 'std_dev', 'std_err'])\n",
    "forward_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_table_max = forward_table['avg_score'].max()\n",
    "forward_table_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forward_table[forward_table['avg_score']==forward_table_max]['feature_names'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward = SFS(model, k_features=1, forward=False, scoring=\"accuracy\", cv = None) #floating=False\n",
    "\n",
    "backward.fit(all_selected_variables, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "backward_table = pd.DataFrame.from_dict(backward.get_metric_dict()).T.drop(columns=['cv_scores', 'ci_bound', 'std_dev', 'std_err'])\n",
    "backward_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_table_max = backward_table['avg_score'].max()\n",
    "backward_table_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "backward_table[backward_table['avg_score']==backward_table_max]['feature_names'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mantaining the variables that appear on both the forward and backward selections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_metric_bf = non_metric_selected.drop(columns=['Higher Education', 'x3_Bachelors', 'x3_Masters', 'x5_5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_selected_variables.drop(columns=['Higher Education', 'x3_Bachelors', 'x3_Masters', 'x5_5'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking point biserial among the selected numerical and categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "print('Point biserial between binary and metric variables:\\n')\n",
    "for i in non_metric_bf.columns:\n",
    "    for j in stand_metric.columns:\n",
    "        pb = pointbiserialr(non_metric_bf[i], stand_metric[j])\n",
    "        \n",
    "        if abs(pb[0]) > 0.5:\n",
    "            print(i, 'and', j, ':', round(pb[0], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#o que acham? kfold vale a pena, fazer as funes e assim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(all_selected_variables,\n",
    "                                                  target,\n",
    "                                                  test_size = 0.3,\n",
    "                                                  random_state = 42,\n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
